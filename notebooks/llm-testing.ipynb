{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e056ac99",
   "metadata": {},
   "source": [
    "## Let's Test Which LLM should be used for Recommending an Astrologer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad44c2c",
   "metadata": {},
   "source": [
    "**Sample LLM client test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "62ddbb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from together import Together\n",
    "client = Together()\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# Response Generation\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"what is the capital of France?\"\n",
    "      }\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849dc221",
   "metadata": {},
   "source": [
    "### Testing Various small sized LLMs on a Custom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4256f100",
   "metadata": {},
   "source": [
    "**Models we will be testing**\n",
    "- meta-llama/Llama-3-8b-chat-hf\n",
    "- Qwen/Qwen2.5-7B-Instruct-Turbo\n",
    "- mistralai/Mistral-7B-Instruct-v0.1\n",
    "- google/gemma-2-27b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46c8ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42118391",
   "metadata": {},
   "source": [
    "**we have created a custom sample dataset along with ground truth to test the LLMs:**\n",
    "\n",
    "Here's a sample - \n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"CASE_001\",\n",
    "  \"user_input\": {\n",
    "    \"profile\": \"Male, 28, Software Engineer\",\n",
    "    \"chat_transcript\": \"Hi, I'm feeling really stuck. My career isn't moving, and I'm thinking of starting my own software company but I'm worried about the financial risk. I just don't know if it's the right move for me.\"\n",
    "  },\n",
    "  \"ideal_recommendation\": {\n",
    "    \"top_3\": [\"Aarav Sharma\", \"Ananya Reddy\", \"Sneha Gupta\"],\n",
    "    \"reasoning\": \"User has a direct career and business question with a financial component, making Aarav Sharma the primary choice. The need for 'Decision Making' about the startup makes Ananya Reddy a strong secondary choice. The general feeling of being 'stuck' relates to 'Life Path & Purpose', making Sneha Gupta relevant for a broader perspective.\"\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5dcf5c",
   "metadata": {},
   "source": [
    "Also, Astrologer's profile data in file - data/astrologers.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98104160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from json file \n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78798d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "astrologers_list = read_json_file(\"data/astrologers.json\")\n",
    "sample_data = read_json_file(\"data/sample_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fdf21c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(user_profile, chat_transcript, astrologer_pool):\n",
    "    \"\"\"Creates a standardized, detailed prompt for the LLM.\"\"\"\n",
    "    \n",
    "    # Format the astrologer pool into a readable string\n",
    "    astrologer_list_str = \"\\n\".join([\n",
    "        f\"- {astro['name']}: Specializes in {', '.join(astro['specialties'])}\"\n",
    "        for astro in astrologer_pool\n",
    "    ])\n",
    "\n",
    "    # The instruction prompt template\n",
    "    prompt = f\"\"\"\n",
    "        You are an expert recommendation engine for an astrology platform. Your task is to analyze a user's profile and chat history to recommend the three most suitable astrologers from a provided list.\n",
    "\n",
    "        **CONTEXT:**\n",
    "        1.  **USER PROFILE:** {user_profile}\n",
    "        2.  **CHAT TRANSCRIPT:** \"{chat_transcript}\"\n",
    "        3.  **AVAILABLE ASTROLOGERS:** \n",
    "        {astrologer_list_str}\n",
    "\n",
    "        **INSTRUCTIONS:**\n",
    "        1.  Carefully read the user's profile and chat transcript to understand their primary concerns.\n",
    "        2.  Match their concerns to the specialties of the available astrologers.\n",
    "        3.  Provide your response as a JSON object containing the top 3 recommended astrologer names and a brief reasoning for your choices.\n",
    "\n",
    "        **Your response MUST strictly be a JSON object in the following format and nothing else:**\n",
    "        {{\n",
    "        \"top_3\": [\"Astrologer Name 1\", \"Astrologer Name 2\", \"Astrologer Name 3\"],\n",
    "        \"reasoning\": \"Your detailed explanation for why these three astrologers were chosen based on the user's issues and the astrologers' specialties.\"\n",
    "        }}\n",
    "    \"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed6c2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = \"meta-llama/Llama-3-8b-chat-hf\"\n",
    "m2 = \"Qwen/Qwen2.5-7B-Instruct-Turbo\"\n",
    "m3 = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "m4 = \"google/gemma-2-27b-it\"\n",
    "# m5 = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5efea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    m2, \n",
    "    m3, \n",
    "    m4\n",
    "]\n",
    "# m1 is already tested previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e23d0669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9ae91216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LLM Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Models:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing Model: Qwen/Qwen2.5-7B-Instruct-Turbo ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Models:  33%|███▎      | 1/3 [00:34<01:08, 34.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Completed Testing Model: Qwen/Qwen2.5-7B-Instruct-Turbo ---\n",
      "--- Testing Model: mistralai/Mistral-7B-Instruct-v0.1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Models:  67%|██████▋   | 2/3 [01:13<00:36, 36.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Completed Testing Model: mistralai/Mistral-7B-Instruct-v0.1 ---\n",
      "--- Testing Model: google/gemma-2-27b-it ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Models: 100%|██████████| 3/3 [01:59<00:00, 39.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Completed Testing Model: google/gemma-2-27b-it ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Execution \n",
    "results = {}\n",
    "print(\"Starting LLM Testing...\")\n",
    "\n",
    "for m in tqdm(models, desc=\"Models\", position=0):\n",
    "    print(f\"--- Testing Model: {m} ---\")\n",
    "    results[m] = {}\n",
    "\n",
    "    for test_case in tqdm(sample_data, desc=\"Test Cases\", position=1, leave=False):\n",
    "        case_id = test_case['id']\n",
    "\n",
    "        # Create the detailed prompt the current test case \n",
    "        prompt_content = create_prompt(\n",
    "            user_profile = test_case[\"user_input\"][\"profile\"], \n",
    "            chat_transcript = test_case[\"user_input\"][\"chat_transcript\"],\n",
    "            astrologer_pool = astrologers_list\n",
    "        )\n",
    "\n",
    "        try: \n",
    "            # LLM Call \n",
    "            response = client.chat.completions.create(\n",
    "                model = m, \n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"user\", \n",
    "                        \"content\": prompt_content\n",
    "                    }\n",
    "                ], \n",
    "                temperature=0.1\n",
    "            )\n",
    "\n",
    "            # Extract the content \n",
    "            response_content = response.choices[0].message.content.strip()\n",
    "            results[m][case_id] = {\n",
    "                \"response\": response_content\n",
    "            }\n",
    "        except Exception as e:\n",
    "            results[m][case_id] = {\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    print(f\"--- Completed Testing Model: {m} ---\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2c7d5550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save \n",
    "with open(\"_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e85e32",
   "metadata": {},
   "source": [
    "## Evaluation:\n",
    "\n",
    "### We have used a highly capable state-of-the-art model **Google's Gemini-2.5-pro** to evaluate the quality of these models\n",
    "\n",
    "The model's output for 20 test cases was evaluated against a pre-defined \"ideal recommendation\" using three core metrics:\n",
    "\n",
    "1.  **Top-1 Accuracy:** Measures if the model's single most important recommendation matches the ideal #1 choice. This tests its ability to identify the most critical expert for the user's primary issue.\n",
    "2.  **Top-3 Overlap Score:** Counts how many of the model's three recommendations appear in the ideal list of three, regardless of their order. This assesses the overall relevance of the suggested astrologers.\n",
    "3.  **Reasoning Quality:** A qualitative score (from 1-3) judging how logical, relevant, and insightful the model's justification for its choices is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01979891",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a93ac",
   "metadata": {},
   "source": [
    "### Evaluation of Recommendation Quality: `meta-llama/Llama-3-8b-chat-hf`\n",
    "\n",
    "This report analyzes the performance of the `meta-llama/Llama-3-8b-chat-hf` model on the task of recommending astrologers based on user profiles and chat transcripts. The evaluation focuses exclusively on the **quality and relevance of the recommendations**, not the specific output format.\n",
    "\n",
    "### Overall Performance Summary\n",
    "\n",
    "**Conclusion:** **Excellent.**\n",
    "\n",
    "The `Llama-3-8B-Instruct` model demonstrates a high degree of competence for this recommendation task. It consistently understands the user's core problems, even when they are nuanced, and matches them to the correct astrologer specialties with impressive accuracy. The model's reasoning is its standout feature, providing clear and logical justifications for its choices.\n",
    "\n",
    "### Quantitative Metrics\n",
    "\n",
    "| Metric | Score | Percentage / Rating |\n",
    "| :--- | :--- | :--- |\n",
    "| **Top-1 Accuracy** | 16 / 20 | 80% |\n",
    "| **Top-3 Overlap** | 44 out of 60 possible | 73.3% |\n",
    "| **Reasoning Quality** | 52 out of 60 possible | 86.7% |\n",
    "\n",
    "### Qualitative Analysis\n",
    "\n",
    "#### Key Strengths:\n",
    "\n",
    "*   **Strong Core Logic:** The model rarely makes an illogical recommendation. Even when its choices differ slightly from the ideal, its reasoning demonstrates a solid understanding of the user's problem.\n",
    "*   **Performance on Clear Cases:** For straightforward user problems, the model performs exceptionally well. In cases like `CASE_007` (needing a Vaastu expert for a new home) or `CASE_020` (a couple facing business and marriage issues), its recommendations were a perfect match with the ideal answer.\n",
    "*   **Excellent Reasoning:** This is the model's greatest strength. It consistently articulates *why* it chose a specific astrologer, correctly connecting user keywords like \"stuck in my career\" to an astrologer's specialty in \"Career & Business\". This builds trust in the recommendation.\n",
    "\n",
    "#### Areas for Improvement:\n",
    "\n",
    "*   **Missing Subtle Keywords:** The model's few errors occurred when it missed a subtle but critical keyword.\n",
    "    *   In `CASE_013`, the user asked, \"**When** will I finally get a job?\" The model recommended career astrologers but missed the urgency of \"when,\" failing to recommend **Mira Kapoor**, the specialist in \"Timing of Events.\"\n",
    "    *   Similarly, in `CASE_014`, the model focused on the legal and family disputes but missed the \"father is **sick**\" cue, overlooking the need for a \"Health & Wellness\" expert.\n",
    "*   **Bias Towards Action-Oriented Advice:** In ambiguous cases like `CASE_006` where the user felt \"lost\" and questioned their \"purpose,\" the model recommended astrologers focused on tangible actions (\"Decision Making,\" \"Career\"). This is a valid interpretation, but it missed the deeper, more introspective options like \"Spiritual Healing\" or \"Past Life Karma\" that were part of the ideal recommendation.\n",
    "\n",
    "### Final Verdict\n",
    "\n",
    "For the core task of generating high-quality, relevant, and well-justified astrologer recommendations, `meta-llama/Llama-3-8b-chat-hf` is a highly effective and robust model. Its performance indicates it is a strong candidate for powering the recommendation engine. The minor issues with subtlety could likely be improved with more advanced prompt engineering or light fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc87ce7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc2ed0d",
   "metadata": {},
   "source": [
    "### Evaluation of Recommendation Quality: `Qwen/Qwen2.5-7B-Instruct-Turbo`\n",
    "\n",
    "This report analyzes the performance of the `Qwen/Qwen2.5-7B-Instruct-Turbo` model on the task of recommending astrologers based on user profiles and chat transcripts. The evaluation focuses exclusively on the quality and relevance of the recommendations.\n",
    "\n",
    "### Overall Performance Summary\n",
    "\n",
    "**Conclusion:** **Fair to Good.**\n",
    "\n",
    "The `Qwen2.5-7B` model demonstrates a foundational ability to handle the recommendation task. It correctly identifies the primary expert in the majority of cases and performs very well on complex problems where the user's needs are explicitly stated across multiple domains (e.g., business and family). However, its performance degrades significantly on queries requiring a deeper, more subtle understanding, where it often recommends irrelevant experts.\n",
    "\n",
    "### Quantitative Metrics\n",
    "\n",
    "| Metric | Score | Percentage / Rating |\n",
    "| :--- | :--- | :--- |\n",
    "| **Top-1 Accuracy** | 15 / 20 | 75% |\n",
    "| **Top-3 Overlap** | 35 out of 60 possible | 58.3% |\n",
    "| **Reasoning Quality** | 39 out of 60 possible | 65% |\n",
    "\n",
    "### Qualitative Analysis\n",
    "\n",
    "#### Key Strengths:\n",
    "\n",
    "*   **Handling Explicit, Multi-faceted Problems:** The model's best performance was on complex cases where the user explicitly mentioned multiple problems. In `CASE_017` (land sale involving finance, family, and property harmony) and `CASE_020` (a couple facing business and marriage issues), the model achieved a perfect score, identifying all correct experts.\n",
    "*   **Identifying the Primary Expert:** The model correctly identified the single most important astrologer in 75% of cases, showing it can generally grasp the user's main issue.\n",
    "*   **Logical Reasoning for Correct Choices:** When the model did select the correct astrologers, its reasoning for those choices was typically logical and well-articulated.\n",
    "\n",
    "#### Areas for Improvement:\n",
    "\n",
    "*   **Difficulty with Nuance and Subtlety:** This is the model's most significant weakness. It repeatedly failed to grasp critical but subtle keywords. For example, it missed \"when\" in `CASE_013` (a query about a job timeline) and the \"more than just physical\" cue in `CASE_019` (a query about unexplained pain), leading to poor recommendations.\n",
    "*   **High Rate of Irrelevant Recommendations:** The model's low `Top-3 Overlap` score (58.3%) highlights a tendency to make irrelevant suggestions. In several cases (`CASE_005`, `CASE_011`, `CASE_018`), after identifying the primary expert, it filled the remaining slots with astrologers whose specialties did not align with the user's problem.\n",
    "*   **Weakness on Introspective Queries:** The model struggled with less tangible, spiritual, or emotional queries. In cases where users felt \"lost\" (`CASE_006`) or were exploring dreams (`CASE_016`), the model's recommendations were often generic and missed the more suitable, specialized experts for introspection and spiritual healing.\n",
    "\n",
    "### Final Verdict\n",
    "\n",
    "The `Qwen/Qwen2.5-7B-Instruct-Turbo` model is a capable baseline but is less reliable than other models tested. Its inability to consistently grasp subtle user needs and its tendency to suggest irrelevant experts make it a less ideal choice for a production system where nuance and high relevance are critical. It would require significant prompt engineering or fine-tuning to overcome these limitations.```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b159ad06",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8dcc9a",
   "metadata": {},
   "source": [
    "### Evaluation of Recommendation Quality: `mixtral-8x7b-instruct-v0.1`\n",
    "\n",
    "This report analyzes the performance of the `mixtral-8x7b-instruct-v0.1` model on the task of recommending astrologers based on user profiles and chat transcripts. The evaluation focuses exclusively on the quality and relevance of the recommendations.\n",
    "\n",
    "### Overall Performance Summary\n",
    "\n",
    "**Conclusion:** **Outstanding.**\n",
    "\n",
    "The `mixtral-8x7b-instruct-v0.1` model demonstrates a superior and highly reliable understanding of the recommendation task. It performs with exceptional accuracy, consistently grasping both the explicit and subtle nuances of the user's needs. Its reasoning is sharp, logical, and directly tied to the provided expert profiles. This model sets the benchmark for quality and relevance.\n",
    "\n",
    "### Quantitative Metrics\n",
    "\n",
    "| Metric | Score | Percentage / Rating |\n",
    "| :--- | :--- | :--- |\n",
    "| **Top-1 Accuracy** | 19 / 20 | 95% |\n",
    "| **Top-3 Overlap** | 56 out of 60 possible | 93.3% |\n",
    "| **Reasoning Quality** | 58 out of 60 possible | 96.7% |\n",
    "\n",
    "### Qualitative Analysis\n",
    "\n",
    "#### Key Strengths:\n",
    "\n",
    "*   **Near-Perfect Accuracy:** The model's ability to identify the correct astrologers is exceptional. With a 95% Top-1 accuracy and a 93.3% Top-3 overlap, its recommendations are consistently the most relevant and helpful. It almost never makes an illogical choice.\n",
    "*   **Deep Nuance Comprehension:** This is the model's most impressive trait. It successfully captures subtle keywords that other models missed. For instance, in `CASE_013` it correctly identified \"When will I finally get a job?\" as a query about **timing** and recommended the \"Timing of Events\" expert. It also correctly identified the \"more than just physical\" nature of the health problem in `CASE_019`.\n",
    "*   **Sharp, Concise, and Relevant Reasoning:** The reasoning provided is consistently of high quality. It is direct, avoids making up justifications, and clearly links the user's specific problem to the astrologer's specialty.\n",
    "*   **Excellent Performance on All Case Types:** The model excels equally on straightforward queries, complex multi-faceted problems, and ambiguous, introspective requests. This versatility makes it highly reliable across the entire spectrum of potential user issues.\n",
    "\n",
    "#### Areas for Improvement:\n",
    "\n",
    "*   **Minimal and Rare Errors:** It is difficult to find significant weaknesses in this model's performance. The very few \"errors\" are minor and often represent a plausible alternative recommendation rather than a mistake. For example, in `CASE_020` (business + marriage issue), its recommendation of a \"Decision Making\" expert instead of a \"Financial Growth\" expert is a different but still highly logical and defensible choice. There are no systemic flaws in its approach.\n",
    "\n",
    "### Final Verdict\n",
    "\n",
    "The `mixtral-8x7b-instruct-v0.1` model is an **excellent and highly recommended choice** for this task. Its combination of high accuracy, nuanced understanding, and logical reasoning makes it the top-performing model evaluated. It can be used with a high degree of confidence to power a trustworthy and effective recommendation engine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vedenv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
